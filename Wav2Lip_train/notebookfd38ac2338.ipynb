{"cells":[{"cell_type":"markdown","metadata":{"editable":false},"source":["### 之前代码有问题，以最新版为准"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["# 任务\n","\n","本次实践涉及到对Wav2Lip模型的，以及相关代码实现。总体上分为以下几个部分：\n","1. 环境的配置\n","2. 数据集准备及预处理\n","3. 模型的训练\n","4. 模型的推理"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["## Wav2Lip\n","**[Wav2Lip](https://arxiv.org/pdf/2008.10010.pdf)** 是一种基于对抗生成网络的由语音驱动的人脸说话视频生成模型。如下图所示，Wav2Lip的网络模型总体上分成三块：生成器、判别器和一个预训练好的Lip-Sync Expert组成。网络的输入有2个：任意的一段视频和一段语音，输出为一段唇音同步的视频。生成器是基于encoder-decoder的网络结构，分别利用2个encoder: speech encoder, identity encoder去对输入的语音和视频人脸进行编码，并将二者的编码结果进行拼接，送入到 face decoder 中进行解码得到输出的视频帧。判别器Visual Quality Discriminator对生成结果的质量进行规范，提高生成视频的清晰度。为了更好的保证生成结果的唇音同步性，Wav2Lip引入了一个预预训练的唇音同步判别模型 Pre-trained Lip-sync Expert，作为衡量生成结果的唇音同步性的额外损失。"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["### Lip-Sync Expert\n","Lip-sync Expert基于 **[SyncNet](https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16a/)**，是一种用来判别语音和视频是否同步的网络模型。如下图所示，SyncNet的输入也是两种：语音特征MFCC和嘴唇的视频帧，利用两个基于卷积神经网络的Encoder分别对输入的语音和视频帧进行降纬和特征提取，将二者的特征都映射到同一个纬度空间中去，最后利用contrastive loss对唇音同步性进行衡量，结果的值越大代表越不同步，结果值越小则代表越同步。在Wav2Lip模型中，进一步改进了SyncNet的网络结构：网络更深；加入了残差网络结构；输入的语音特征被替换成了mel-spectrogram特征。"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["## 1. 环境的配置\n","- `建议准备一台有显卡的linux系统电脑，或者可以选择使用第三方云服务器（Google Colab）` \n","- `Python 3.6 或者更高版本` \n","- ffmpeg: `sudo apt-get install ffmpeg`\n","- 必要的python包的安装，所需要的库名称都已经包含在`requirements.txt`文件中，可以使用 `pip install -r requirements.txt`一次性安装. \n","- 在本实验中利用到了人脸检测的相关技术，需要下载人脸检测预训练模型：Face detection [pre-trained model](https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth) 并移动到 `face_detection/detection/sfd/s3fd.pth`文件夹下. "]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false},"outputs":[],"source":["# !pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["## 2. 数据集的准备及预处理\n","\n","**LRS2 数据集的下载**  \n","实验所需要的数据集下载地址为：<a href=\"http://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html\">LRS2 dataset</a>，下载该数据集需要获得BBC的许可，需要发送申请邮件以获取下载密钥，具体操作详见网页中的指示。下载完成后对数据集进行解压到本目录的`mvlrs_v1/`文件夹下，并将LRS2中的文件列表文件`train.txt, val.txt, test.txt` 移动到`filelists/`文件夹下，最终得到的数据集目录结构如下所示。\n","```\n","data_root (mvlrs_v1)\n","├── main, pretrain (我们只使用main文件夹下的数据)\n","|\t├── 文件夹列表\n","|\t│   ├── 5位以.mp4结尾的视频ID\n","```\n","**数据集预处理**\n","数据集中大多数视频都是包含人的半身或者全身的画面，而我们的模型只需要人脸这一小部分。所以在预处理阶段，我们要对每一个视频进行分帧操作，提取视频的每一帧，之后使用`face detection`工具包对人脸位置进行定位并裁减，只保留人脸的图片帧。同时，我们也需要将每一个视频中的语音分离出来。"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false},"outputs":[],"source":["# !python preprocess.py --data_root \"./mvlrs_v1/main\" --preprocessed_root \"./lrs2_preprocessed\" "]},{"cell_type":"markdown","metadata":{"editable":false},"source":["预处理后的`lrs2_preprocessed/`文件夹下的目录结构如下\n","```\n","preprocessed_root (lrs2_preprocessed)\n","├── 文件夹列表\n","|\t├── 五位的视频ID\n","|\t│   ├── *.jpg\n","|\t│   ├── audio.wav\n","```"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["## 3. 模型训练\n","模型的训练主要分为两个部分：\n","1. Lip-Sync Expert Discriminator的训练。这里提供官方的预训练模型 [weight](https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EQRvmiZg-HRAjvI6zqN9eTEBP74KefynCwPWVmF57l-AYA?e=ZRPHKP)\n","2. Wav2Lip 模型的训练。\n","\n","### 3.1 预训练Lip-Sync Expert\n","#### 1. 网络的搭建 \n","上面我们已经介绍了SyncNet的基本网络结构，主要有一系列的(Conv+BatchNorm+Relu)组成，这里我们对其进行了一些改进，加入了残差结构。为了方便之后的使用，我们对(Conv+BatchNorm+Relu)以及残差模块进行了封装。"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-30T15:10:55.205665Z","iopub.status.busy":"2021-07-30T15:10:55.205198Z","iopub.status.idle":"2021-07-30T15:10:55.224198Z","shell.execute_reply":"2021-07-30T15:10:55.222781Z","shell.execute_reply.started":"2021-07-30T15:10:55.205557Z"},"trusted":true},"outputs":[],"source":["%cd ../input/wave2lip/wav2lip_homework"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-30T15:10:59.61032Z","iopub.status.busy":"2021-07-30T15:10:59.609904Z","iopub.status.idle":"2021-07-30T15:11:01.327087Z","shell.execute_reply":"2021-07-30T15:11:01.326036Z","shell.execute_reply.started":"2021-07-30T15:10:59.610288Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","\n","class Conv2d(nn.Module):\n","    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        ########TODO######################\n","        # 按下面的网络结构要求，补全代码\n","        # self.conv_block: Sequential结构，Conv2d+BatchNorm\n","        # self.act: relu激活函数\n","        self.conv_block = nn.Sequential(\n","                            nn.Conv2d(cin, cout, kernel_size, stride, padding),\n","                            nn.BatchNorm2d(cout)\n","                            )\n","        self.act = nn.ReLU()\n","        self.residual = residual\n","\n","    def forward(self, x):\n","        out = self.conv_block(x)\n","        if self.residual:\n","            out += x\n","        return self.act(out)"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["SyncNet的主要包含两个部分：Face_encoder和Audio_encoder。每一个部分都由多个Conv2d模块组成，通过指定卷积核的大小实现对输入的下采样和特征提取"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-30T15:11:04.533418Z","iopub.status.busy":"2021-07-30T15:11:04.53297Z","iopub.status.idle":"2021-07-30T15:11:04.559706Z","shell.execute_reply":"2021-07-30T15:11:04.558289Z","shell.execute_reply.started":"2021-07-30T15:11:04.53338Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","\n","class SyncNet_color(nn.Module):\n","    def __init__(self):\n","        super(SyncNet_color, self).__init__()\n","        \n","        ################TODO###################\n","        #根据上面提供的网络结构图，补全下面卷积网络的参数\n","\n","        self.face_encoder = nn.Sequential(\n","            Conv2d(15, 32, kernel_size=(7, 7), stride=1, padding=3),\n","\n","            Conv2d(32, 64, kernel_size=5, stride=(1, 2), padding=1),\n","            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n","\n","            Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n","            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n","\n","            Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n","            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n","\n","            Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n","            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n","\n","            Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n","            Conv2d(512, 512, kernel_size=3, stride=1, padding=0),\n","            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n","\n","        self.audio_encoder = nn.Sequential(\n","            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n","            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n","\n","            Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),\n","            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n","\n","            Conv2d(64, 128, kernel_size=3, stride=3, padding=1),\n","            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n","\n","            Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),\n","            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n","\n","            Conv2d(256, 512, kernel_size=3, stride=1, padding=0),\n","            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n","\n","    def forward(self, audio_sequences, face_sequences): # audio_sequences := (B, dim, T)\n","        \n","        #########################TODO#######################\n","        # 正向传播\n","        face_embedding = self.face_encoder(face_sequences)\n","        audio_embedding = self.audio_encoder(audio_sequences)\n","\n","        audio_embedding = audio_embedding.view(audio_embedding.size(0), -1)\n","        face_embedding = face_embedding.view(face_embedding.size(0), -1)\n","\n","        audio_embedding = F.normalize(audio_embedding, p=2, dim=1)\n","        face_embedding = F.normalize(face_embedding, p=2, dim=1)\n","\n","\n","        return audio_embedding, face_embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-30T15:11:11.303557Z","iopub.status.busy":"2021-07-30T15:11:11.30313Z","iopub.status.idle":"2021-07-30T15:11:14.109884Z","shell.execute_reply":"2021-07-30T15:11:14.108416Z","shell.execute_reply.started":"2021-07-30T15:11:11.303526Z"},"trusted":true},"outputs":[],"source":["from os.path import dirname, join, basename, isfile\n","from tqdm import tqdm\n","\n","from models import SyncNet_color as SyncNet\n","import audio\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.backends.cudnn as cudnn\n","from torch.utils import data as data_utils\n","import numpy as np\n","\n","from glob import glob\n","\n","import os, random, cv2, argparse\n","from hparams import hparams, get_image_list"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["#### 2.数据集的定义"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-30T15:11:14.929449Z","iopub.status.busy":"2021-07-30T15:11:14.929028Z","iopub.status.idle":"2021-07-30T15:11:15.007361Z","shell.execute_reply":"2021-07-30T15:11:15.006074Z","shell.execute_reply.started":"2021-07-30T15:11:14.929415Z"},"trusted":true},"outputs":[],"source":["global_step = 0 #起始的step\n","global_epoch = 0 #起始的epoch\n","use_cuda = torch.cuda.is_available()#训练的设备 cpu or gpu\n","print('use_cuda: {}'.format(use_cuda))\n","\n","syncnet_T = 5 ## 每次选取200ms的视频片段进行训练，视频的fps为25，所以200ms对应的帧数为：25*0.2=5帧\n","syncnet_mel_step_size = 16 # 200ms对应的声音的mel-spectrogram特征的长度为16.\n","data_root=\"/kaggle/input/wav2lippreprocessed/lrs2_preprocessed\" #数据集的位置"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-30T15:18:29.738572Z","iopub.status.busy":"2021-07-30T15:18:29.738178Z","iopub.status.idle":"2021-07-30T15:18:29.759882Z","shell.execute_reply":"2021-07-30T15:18:29.758164Z","shell.execute_reply.started":"2021-07-30T15:18:29.73854Z"},"trusted":true},"outputs":[],"source":["class Dataset(object):\n","    def __init__(self, split):\n","        self.all_videos = get_image_list(data_root, split)\n","\n","    def get_frame_id(self, frame):\n","        return int(basename(frame).split('.')[0])\n","\n","    def get_window(self, start_frame):\n","        start_id = self.get_frame_id(start_frame)\n","        vidname = dirname(start_frame)\n","\n","        window_fnames = []\n","        for frame_id in range(start_id, start_id + syncnet_T):\n","            frame = join(vidname, '{}.jpg'.format(frame_id))\n","            if not isfile(frame):\n","                return None\n","            window_fnames.append(frame)\n","        return window_fnames\n","\n","    def crop_audio_window(self, spec, start_frame):\n","        # num_frames = (T x hop_size * fps) / sample_rate\n","        start_frame_num = self.get_frame_id(start_frame)\n","        start_idx = int(80. * (start_frame_num / float(hparams.fps)))\n","\n","        end_idx = start_idx + syncnet_mel_step_size\n","\n","        return spec[start_idx : end_idx, :]\n","\n","\n","    def __len__(self):\n","        return len(self.all_videos)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        return: x,mel,y\n","        x: 五张嘴唇图片\n","        mel: 对应的语音的mel spectrogram\n","        t: 同步or不同步\n","        \n","        \"\"\"\n","        while 1:\n","            idx = random.randint(0, len(self.all_videos) - 1)\n","            vidname = self.all_videos[idx]\n","\n","            img_names = list(glob(join(vidname, '*.jpg')))\n","            if len(img_names) <= 3 * syncnet_T:\n","                continue\n","            img_name = random.choice(img_names)\n","            wrong_img_name = random.choice(img_names)\n","            while wrong_img_name == img_name:\n","                wrong_img_name = random.choice(img_names)\n","            \n","            \n","            #随机决定是产生负样本还是正样本\n","            if random.choice([True, False]):\n","                y = torch.ones(1).float()\n","                chosen = img_name\n","            else:\n","                y = torch.zeros(1).float()\n","                chosen = wrong_img_name\n","\n","            window_fnames = self.get_window(chosen)\n","            if window_fnames is None:\n","                continue\n","\n","            window = []\n","            all_read = True\n","            for fname in window_fnames:\n","                img = cv2.imread(fname)\n","                if img is None:\n","                    all_read = False\n","                    break\n","                try:\n","                    img = cv2.resize(img, (hparams.img_size, hparams.img_size))\n","                except Exception as e:\n","                    all_read = False\n","                    break\n","\n","                window.append(img)\n","\n","            if not all_read: continue\n","\n","            try:\n","                wavpath = join(vidname, \"audio.wav\")\n","                wav = audio.load_wav(wavpath, hparams.sample_rate)\n","\n","                orig_mel = audio.melspectrogram(wav).T\n","            except Exception as e:\n","                continue\n","\n","            mel = self.crop_audio_window(orig_mel.copy(), img_name)\n","\n","            if (mel.shape[0] != syncnet_mel_step_size):\n","                continue\n","\n","            # H x W x 3 * T\n","            x = np.concatenate(window, axis=2) / 255.\n","            x = x.transpose(2, 0, 1)\n","            x = x[:, x.shape[1]//2:]\n","\n","            x = torch.FloatTensor(x)\n","            mel = torch.FloatTensor(mel.T).unsqueeze(0)\n","\n","            return x, mel, y"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-30T15:18:33.470465Z","iopub.status.busy":"2021-07-30T15:18:33.470113Z","iopub.status.idle":"2021-07-30T15:18:33.783955Z","shell.execute_reply":"2021-07-30T15:18:33.782592Z","shell.execute_reply.started":"2021-07-30T15:18:33.470434Z"},"trusted":true},"outputs":[],"source":["ds=Dataset(\"train\")\n","x,mel,t=ds[0]\n","print(x.shape)\n","print(mel.shape)\n","print(t.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-30T15:18:36.93343Z","iopub.status.busy":"2021-07-30T15:18:36.932952Z","iopub.status.idle":"2021-07-30T15:18:37.134064Z","shell.execute_reply":"2021-07-30T15:18:37.132739Z","shell.execute_reply.started":"2021-07-30T15:18:36.933382Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.imshow(mel[0].numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-30T15:18:56.039132Z","iopub.status.busy":"2021-07-30T15:18:56.038747Z","iopub.status.idle":"2021-07-30T15:18:56.211108Z","shell.execute_reply":"2021-07-30T15:18:56.209869Z","shell.execute_reply.started":"2021-07-30T15:18:56.039099Z"},"trusted":true},"outputs":[],"source":["plt.imshow(x[:3,:,:].transpose(0,2).numpy())"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["#### 3.训练\n","\n","使用cosine_loss 作为损失函数"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-30T15:19:00.67425Z","iopub.status.busy":"2021-07-30T15:19:00.673772Z","iopub.status.idle":"2021-07-30T15:19:00.69968Z","shell.execute_reply":"2021-07-30T15:19:00.694769Z","shell.execute_reply.started":"2021-07-30T15:19:00.674203Z"},"trusted":true},"outputs":[],"source":["#损失函数的定义\n","logloss = nn.BCELoss() # 交叉熵损失\n","def cosine_loss(a, v, y):#余弦相似度损失\n","    \"\"\"\n","    a: audio_encoder的输出\n","    v: video face_encoder的输出\n","    y: 是否同步的真实值\n","    \"\"\"\n","    d = nn.functional.cosine_similarity(a, v)\n","    loss = logloss(d.unsqueeze(1), y)\n","\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-30T15:19:09.309426Z","iopub.status.busy":"2021-07-30T15:19:09.309046Z","iopub.status.idle":"2021-07-30T15:19:09.330578Z","shell.execute_reply":"2021-07-30T15:19:09.329271Z","shell.execute_reply.started":"2021-07-30T15:19:09.309393Z"},"trusted":true},"outputs":[],"source":["def train(device, model, train_data_loader, test_data_loader, optimizer,\n","          checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n","\n","    global global_step, global_epoch\n","    resumed_step = global_step\n","    \n","    while global_epoch < nepochs:\n","        running_loss = 0.\n","        prog_bar = tqdm(enumerate(train_data_loader))\n","        for step, (x, mel, y) in prog_bar:\n","            model.train()\n","            optimizer.zero_grad()\n","\n","            #####TODO###########\n","            ####################\n","            #补全模型的训练\n","            x = x.to(device)\n","\n","            mel = mel.to(device)\n","\n","            a, v = model(mel, x)\n","            y = y.to(device)\n","\n","            loss = cosine_loss(a, v, y)\n","            loss.backward()\n","            optimizer.step()\n","                \n","            \n","\n","            global_step += 1\n","            cur_session_steps = global_step - resumed_step\n","            running_loss += loss.item()\n","\n","            if global_step == 1 or global_step % checkpoint_interval == 0:\n","                save_checkpoint(\n","                    model, optimizer, global_step, checkpoint_dir, global_epoch)\n","\n","            if global_step % hparams.syncnet_eval_interval == 0:\n","                with torch.no_grad():\n","                    eval_model(test_data_loader, global_step, device, model, checkpoint_dir)\n","\n","            prog_bar.set_description('Epoch: {} Loss: {}'.format(global_epoch, running_loss / (step + 1)))\n","\n","        global_epoch += 1\n","\n","def eval_model(test_data_loader, global_step, device, model, checkpoint_dir):\n","    #在测试集上进行评估\n","    eval_steps = 1400\n","    print('Evaluating for {} steps'.format(eval_steps))\n","    losses = []\n","    while 1:\n","        for step, (x, mel, y) in enumerate(test_data_loader):\n","\n","            model.eval()\n","\n","            # Transform data to CUDA device\n","            x = x.to(device)\n","\n","            mel = mel.to(device)\n","\n","            a, v = model(mel, x)\n","            y = y.to(device)\n","\n","            loss = cosine_loss(a, v, y)\n","            losses.append(loss.item())\n","\n","            if step > eval_steps: break\n","\n","        averaged_loss = sum(losses) / len(losses)\n","        print(averaged_loss)\n","\n","        return\n","\n","latest_checkpoint_path = ''\n","def save_checkpoint(model, optimizer, step, checkpoint_dir, epoch):\n","    #保存训练的结果 checkpoint\n","    global latest_checkpoint_path\n","    \n","    checkpoint_path = join(\n","        checkpoint_dir, \"checkpoint_step{:09d}.pth\".format(global_step))\n","    optimizer_state = optimizer.state_dict() if hparams.save_optimizer_state else None\n","    torch.save({\n","        \"state_dict\": model.state_dict(),\n","        \"optimizer\": optimizer_state,\n","        \"global_step\": step,\n","        \"global_epoch\": epoch,\n","    }, checkpoint_path)\n","    latest_checkpoint_path = checkpoint_path\n","    print(\"Saved checkpoint:\", checkpoint_path)\n","\n","def _load(checkpoint_path):\n","    if use_cuda:\n","        checkpoint = torch.load(checkpoint_path)\n","    else:\n","        checkpoint = torch.load(checkpoint_path,\n","                                map_location=lambda storage, loc: storage)\n","    return checkpoint\n","\n","def load_checkpoint(path, model, optimizer, reset_optimizer=False):\n","    #读取指定checkpoint的保存信息\n","    global global_step\n","    global global_epoch\n","\n","    print(\"Load checkpoint from: {}\".format(path))\n","    checkpoint = _load(path)\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    if not reset_optimizer:\n","        optimizer_state = checkpoint[\"optimizer\"]\n","        if optimizer_state is not None:\n","            print(\"Load optimizer state from {}\".format(path))\n","            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","    global_step = checkpoint[\"global_step\"]\n","    global_epoch = checkpoint[\"global_epoch\"]\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["**下面开始训练，最终的Loss参考值为0.20左右，此时模型能达到较好的判别效果**"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-30T15:19:14.215432Z","iopub.status.busy":"2021-07-30T15:19:14.21506Z","iopub.status.idle":"2021-07-30T15:22:57.894Z","shell.execute_reply":"2021-07-30T15:22:57.892197Z","shell.execute_reply.started":"2021-07-30T15:19:14.2154Z"},"trusted":true},"outputs":[],"source":["checkpoint_dir = \"/kaggle/working/expert_checkpoints/\" #指定存储 checkpoint的位置\n","checkpoint_path = '/kaggle/input/wav2lip24epoch/expert_checkpoints/checkpoint_step000060000.pth'\n","# 指定加载checkpoint的路径，第一次训练时不需要，后续如果想从某个checkpoint恢复训练，可指定。\n","\n","if not os.path.exists(checkpoint_dir): os.mkdir(checkpoint_dir)\n","\n","# Dataset and Dataloader setup\n","train_dataset = Dataset('train')\n","test_dataset = Dataset('val')\n","\n","############TODO#########\n","#####Train Dataloader and Test Dataloader \n","#### 具体的bacthsize等参数，参考 hparams.py文件\n","train_data_loader = data_utils.DataLoader(\n","    train_dataset, batch_size=hparams.batch_size, shuffle=True,\n","    num_workers=hparams.num_workers)\n","\n","test_data_loader = data_utils.DataLoader(\n","    test_dataset, batch_size=hparams.batch_size,\n","    num_workers=8)\n","\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","# Model\n","#####定义 SynNet模型，并加载到指定的device上\n","model = SyncNet().to(device)\n","print('total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n","\n","####定义优化器，使用adam，lr参考hparams.py文件\n","optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n","                       lr=1e-5)\n","\n","if checkpoint_path is not None:\n","    load_checkpoint(checkpoint_path, model, optimizer, reset_optimizer=True)\n","\n","train(device, model, train_data_loader, test_data_loader, optimizer,\n","      checkpoint_dir=checkpoint_dir,\n","      checkpoint_interval=hparams.syncnet_checkpoint_interval,\n","      nepochs=27)"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["### 3.2 训练Wav2Lip\n","预训练模型 [weight](https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA?e=n9ljGW)\n","#### 1. 模型的定义\n","wav2lip模型的生成器首先对输入进行下采样，然后再经过上采样恢复成原来的大小。为了方便，我们对其中重复利用到的模块进行了封装。"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-25T08:00:59.236643Z","iopub.status.busy":"2021-07-25T08:00:59.236262Z","iopub.status.idle":"2021-07-25T08:00:59.244965Z","shell.execute_reply":"2021-07-25T08:00:59.244116Z","shell.execute_reply.started":"2021-07-25T08:00:59.2366Z"},"trusted":true},"outputs":[],"source":["class nonorm_Conv2d(nn.Module): #不需要进行 norm的卷积\n","    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.conv_block = nn.Sequential(\n","                            nn.Conv2d(cin, cout, kernel_size, stride, padding),\n","                            )\n","        self.act = nn.LeakyReLU(0.01, inplace=True)\n","\n","    def forward(self, x):\n","        out = self.conv_block(x)\n","        return self.act(out)\n","\n","class Conv2dTranspose(nn.Module):# 逆卷积，上采样\n","    def __init__(self, cin, cout, kernel_size, stride, padding, output_padding=0, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        ############TODO###########\n","        ## 完成self.conv_block: 一个逆卷积和batchnorm组成的 Sequential结构\n","        self.conv_block = nn.Sequential(\n","                            nn.ConvTranspose2d(cin, cout, kernel_size, stride, padding, output_padding),\n","                            nn.BatchNorm2d(cout)\n","                            )\n","        self.act = nn.ReLU()\n","\n","    def forward(self, x):\n","        out = self.conv_block(x)\n","        return self.act(out)"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["**生成器**  \n","由两个encoder: face_encoder和 audio_encoder, 一个decoder：face_decoder组成。face encoder 和 audio encoder 分别对输入的人脸和语音特征进行降维，得到（1，1，512）的特征，并将二者进行拼接送入到 face decoder中去进行上采样，最终得到和输入一样大小的人脸图像。"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-25T08:00:04.850072Z","iopub.status.busy":"2021-07-25T08:00:04.849691Z","iopub.status.idle":"2021-07-25T08:00:04.886353Z","shell.execute_reply":"2021-07-25T08:00:04.884997Z","shell.execute_reply.started":"2021-07-25T08:00:04.850039Z"},"trusted":true},"outputs":[],"source":["\n","#####################TODO############################\n","#根据下面打印的网络模型图，补全网络的参数\n","\n","class Wav2Lip(nn.Module):\n","    def __init__(self):\n","        super(Wav2Lip, self).__init__()\n","\n","        self.face_encoder_blocks = nn.ModuleList([\n","            nn.Sequential(Conv2d(6, 16, kernel_size=7, stride=1, padding=3)), # 96,96\n","\n","            nn.Sequential(Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # 48,48\n","            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True)),\n","\n","            nn.Sequential(Conv2d(32, 64, kernel_size=3, stride=2, padding=1),    # 24,24\n","            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True)),\n","\n","            nn.Sequential(Conv2d(64, 128, kernel_size=3, stride=2, padding=1),   # 12,12\n","            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True)),\n","\n","            nn.Sequential(Conv2d(128, 256, kernel_size=3, stride=2, padding=1),       # 6,6\n","            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True)),\n","\n","            nn.Sequential(Conv2d(256, 512, kernel_size=3, stride=2, padding=1),     # 3,3\n","            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),),\n","            \n","            nn.Sequential(Conv2d(512, 512, kernel_size=3, stride=1, padding=0),     # 1, 1\n","            Conv2d(512, 512, kernel_size=1, stride=1, padding=0)),])\n","\n","        self.audio_encoder = nn.Sequential(\n","            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n","            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n","\n","            Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),\n","            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n","\n","            Conv2d(64, 128, kernel_size=3, stride=3, padding=1),\n","            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n","\n","            Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),\n","            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n","\n","            Conv2d(256, 512, kernel_size=3, stride=1, padding=0),\n","            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n","\n","        self.face_decoder_blocks = nn.ModuleList([\n","            nn.Sequential(Conv2d(512, 512, kernel_size=1, stride=1, padding=0),),\n","\n","            nn.Sequential(Conv2dTranspose(1024, 512, kernel_size=3, stride=1, padding=0), # 3,3\n","            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),),\n","\n","            nn.Sequential(Conv2dTranspose(1024, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),), # 6, 6\n","\n","            nn.Sequential(Conv2dTranspose(768, 384, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            Conv2d(384, 384, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(384, 384, kernel_size=3, stride=1, padding=1, residual=True),), # 12, 12\n","\n","            nn.Sequential(Conv2dTranspose(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),), # 24, 24\n","\n","            nn.Sequential(Conv2dTranspose(320, 128, kernel_size=3, stride=2, padding=1, output_padding=1), \n","            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),), # 48, 48\n","\n","            nn.Sequential(Conv2dTranspose(160, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n","            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),),]) # 96,96\n","\n","        self.output_block = nn.Sequential(Conv2d(80, 32, kernel_size=3, stride=1, padding=1),\n","            nn.Conv2d(32, 3, kernel_size=1, stride=1, padding=0),\n","            nn.Sigmoid()) \n","\n","    def forward(self, audio_sequences, face_sequences):\n","        # audio_sequences = (B, T, 1, 80, 16)\n","        B = audio_sequences.size(0)\n","\n","        input_dim_size = len(face_sequences.size())\n","        if input_dim_size > 4:\n","            audio_sequences = torch.cat([audio_sequences[:, i] for i in range(audio_sequences.size(1))], dim=0)\n","            face_sequences = torch.cat([face_sequences[:, :, i] for i in range(face_sequences.size(2))], dim=0)\n","\n","        audio_embedding = self.audio_encoder(audio_sequences) # B, 512, 1, 1\n","\n","        feats = []\n","        x = face_sequences\n","        for f in self.face_encoder_blocks:\n","            x = f(x)\n","            feats.append(x)\n","\n","        x = audio_embedding\n","        for f in self.face_decoder_blocks:\n","            x = f(x)\n","            try:\n","                x = torch.cat((x, feats[-1]), dim=1)\n","            except Exception as e:\n","                print(x.size())\n","                print(feats[-1].size())\n","                raise e\n","            \n","            feats.pop()\n","\n","        x = self.output_block(x)\n","\n","        if input_dim_size > 4:\n","            x = torch.split(x, B, dim=0) # [(B, C, H, W)]\n","            outputs = torch.stack(x, dim=2) # (B, C, T, H, W)\n","\n","        else:\n","            outputs = x\n","            \n","        return outputs"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["**判别器**  \n","判别器也是由一系列的卷积神经网络组成，输入一张人脸图片，利用face encoder对其进行降维到512维。"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-25T08:00:08.96179Z","iopub.status.busy":"2021-07-25T08:00:08.961312Z","iopub.status.idle":"2021-07-25T08:00:08.979378Z","shell.execute_reply":"2021-07-25T08:00:08.978282Z","shell.execute_reply.started":"2021-07-25T08:00:08.961755Z"},"trusted":true},"outputs":[],"source":["\n","###########TODO##################\n","####补全判别器模型\n","class Wav2Lip_disc_qual(nn.Module):\n","    def __init__(self):\n","        super(Wav2Lip_disc_qual, self).__init__()\n","\n","        self.face_encoder_blocks = nn.ModuleList([\n","            nn.Sequential(nonorm_Conv2d(3, 32, kernel_size=7, stride=1, padding=3)), # 48,96\n","\n","            nn.Sequential(nonorm_Conv2d(32, 64, kernel_size=5, stride=(1, 2), padding=2), # 48,48\n","            nonorm_Conv2d(64, 64, kernel_size=5, stride=1, padding=2)),\n","\n","            nn.Sequential(nonorm_Conv2d(64, 128, kernel_size=5, stride=2, padding=2),    # 24,24\n","            nonorm_Conv2d(128, 128, kernel_size=5, stride=1, padding=2)),\n","\n","            nn.Sequential(nonorm_Conv2d(128, 256, kernel_size=5, stride=2, padding=2),   # 12,12\n","            nonorm_Conv2d(256, 256, kernel_size=5, stride=1, padding=2)),\n","\n","            nn.Sequential(nonorm_Conv2d(256, 512, kernel_size=3, stride=2, padding=1),       # 6,6\n","            nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=1)),\n","\n","            nn.Sequential(nonorm_Conv2d(512, 512, kernel_size=3, stride=2, padding=1),     # 3,3\n","            nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=1),),\n","            \n","            nn.Sequential(nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=0),     # 1, 1\n","            nonorm_Conv2d(512, 512, kernel_size=1, stride=1, padding=0)),])\n","\n","        self.binary_pred = nn.Sequential(nn.Conv2d(512, 1, kernel_size=1, stride=1, padding=0), nn.Sigmoid())\n","        self.label_noise = .0\n","\n","    def get_lower_half(self, face_sequences):\n","        return face_sequences[:, :, face_sequences.size(2)//2:]\n","\n","    def to_2d(self, face_sequences):\n","        B = face_sequences.size(0)\n","        face_sequences = torch.cat([face_sequences[:, :, i] for i in range(face_sequences.size(2))], dim=0)\n","        return face_sequences\n","\n","    def perceptual_forward(self, false_face_sequences):\n","        false_face_sequences = self.to_2d(false_face_sequences)\n","        false_face_sequences = self.get_lower_half(false_face_sequences)\n","\n","        false_feats = false_face_sequences\n","        for f in self.face_encoder_blocks:\n","            false_feats = f(false_feats)\n","\n","        false_pred_loss = F.binary_cross_entropy(self.binary_pred(false_feats).view(len(false_feats), -1), \n","                                        torch.ones((len(false_feats), 1)).cuda())\n","\n","        return false_pred_loss\n","\n","    def forward(self, face_sequences):\n","        face_sequences = self.to_2d(face_sequences)\n","        face_sequences = self.get_lower_half(face_sequences)\n","\n","        x = face_sequences\n","        for f in self.face_encoder_blocks:\n","            x = f(x)\n","\n","        return self.binary_pred(x).view(len(x), -1)"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["#### 2. 数据集的定义  \n","在训练时，会用到4个数据：\n","1. x:输入的图片\n","2. indiv_mels: 每一张图片所对应语音的mel-spectrogram特征\n","3. mel: 所有帧对应的200ms的语音mel-spectrogram，用于SyncNet进行唇音同步损失的计算\n","4. y:真实的与语音对应的，唇音同步的图片。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-25T08:00:11.621419Z","iopub.status.busy":"2021-07-25T08:00:11.62105Z","iopub.status.idle":"2021-07-25T08:00:11.65724Z","shell.execute_reply":"2021-07-25T08:00:11.655984Z","shell.execute_reply.started":"2021-07-25T08:00:11.621386Z"},"trusted":true},"outputs":[],"source":["global_step = 0\n","global_epoch = 0\n","use_cuda = torch.cuda.is_available()\n","print('use_cuda: {}'.format(use_cuda))\n","\n","syncnet_T = 5\n","syncnet_mel_step_size = 16\n","\n","class Dataset(object):\n","    def __init__(self, split):\n","        self.all_videos = get_image_list(data_root, split)\n","\n","    def get_frame_id(self, frame):\n","        return int(basename(frame).split('.')[0])\n","\n","    def get_window(self, start_frame):\n","        start_id = self.get_frame_id(start_frame)\n","        vidname = dirname(start_frame)\n","\n","        window_fnames = []\n","        for frame_id in range(start_id, start_id + syncnet_T):\n","            frame = join(vidname, '{}.jpg'.format(frame_id))\n","            if not isfile(frame):\n","                return None\n","            window_fnames.append(frame)\n","        return window_fnames\n","\n","    def read_window(self, window_fnames):\n","        if window_fnames is None: return None\n","        window = []\n","        for fname in window_fnames:\n","            img = cv2.imread(fname)\n","            if img is None:\n","                return None\n","            try:\n","                img = cv2.resize(img, (hparams.img_size, hparams.img_size))\n","            except Exception as e:\n","                return None\n","\n","            window.append(img)\n","\n","        return window\n","\n","    def crop_audio_window(self, spec, start_frame):\n","        if type(start_frame) == int:\n","            start_frame_num = start_frame\n","        else:\n","            start_frame_num = self.get_frame_id(start_frame) # 0-indexing ---> 1-indexing\n","        start_idx = int(80. * (start_frame_num / float(hparams.fps)))\n","        \n","        end_idx = start_idx + syncnet_mel_step_size\n","\n","        return spec[start_idx : end_idx, :]\n","\n","    def get_segmented_mels(self, spec, start_frame):\n","        mels = []\n","        assert syncnet_T == 5\n","        start_frame_num = self.get_frame_id(start_frame) + 1 # 0-indexing ---> 1-indexing\n","        if start_frame_num - 2 < 0: return None\n","        for i in range(start_frame_num, start_frame_num + syncnet_T):\n","            m = self.crop_audio_window(spec, i - 2)\n","            if m.shape[0] != syncnet_mel_step_size:\n","                return None\n","            mels.append(m.T)\n","\n","        mels = np.asarray(mels)\n","\n","        return mels\n","\n","    def prepare_window(self, window):\n","        # 3 x T x H x W\n","        x = np.asarray(window) / 255.\n","        x = np.transpose(x, (3, 0, 1, 2))\n","\n","        return x\n","\n","    def __len__(self):\n","        return len(self.all_videos)\n","\n","    def __getitem__(self, idx):\n","        while 1:\n","            idx = random.randint(0, len(self.all_videos) - 1) #随机选择一个视频id\n","            vidname = self.all_videos[idx]\n","            img_names = list(glob(join(vidname, '*.jpg')))\n","            if len(img_names) <= 3 * syncnet_T:\n","                continue\n","            \n","            img_name = random.choice(img_names)\n","            wrong_img_name = random.choice(img_names)#随机选择帧\n","            while wrong_img_name == img_name:\n","                wrong_img_name = random.choice(img_names)\n","\n","            window_fnames = self.get_window(img_name)\n","            wrong_window_fnames = self.get_window(wrong_img_name)\n","            if window_fnames is None or wrong_window_fnames is None:\n","                continue\n","\n","            window = self.read_window(window_fnames)\n","            if window is None:\n","                continue\n","\n","            wrong_window = self.read_window(wrong_window_fnames)\n","            if wrong_window is None:\n","                continue\n","\n","            try:\n","                #读取音频\n","                wavpath = join(vidname, \"audio.wav\")\n","                wav = audio.load_wav(wavpath, hparams.sample_rate)\n","                #提取完整mel-spectrogram\n","                orig_mel = audio.melspectrogram(wav).T\n","            except Exception as e:\n","                continue\n","            # 分割 mel-spectrogram\n","            mel = self.crop_audio_window(orig_mel.copy(), img_name)\n","            \n","            if (mel.shape[0] != syncnet_mel_step_size):\n","                continue\n","\n","            indiv_mels = self.get_segmented_mels(orig_mel.copy(), img_name)\n","            if indiv_mels is None: continue\n","\n","            window = self.prepare_window(window)\n","            y = window.copy()\n","            window[:, :, window.shape[2]//2:] = 0.\n","\n","            wrong_window = self.prepare_window(wrong_window)\n","            x = np.concatenate([window, wrong_window], axis=0)\n","\n","            x = torch.FloatTensor(x)\n","            mel = torch.FloatTensor(mel.T).unsqueeze(0)\n","            indiv_mels = torch.FloatTensor(indiv_mels).unsqueeze(1)\n","            y = torch.FloatTensor(y)\n","            return x, indiv_mels, mel, y"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-25T08:00:15.736852Z","iopub.status.busy":"2021-07-25T08:00:15.736475Z","iopub.status.idle":"2021-07-25T08:00:15.978544Z","shell.execute_reply":"2021-07-25T08:00:15.977464Z","shell.execute_reply.started":"2021-07-25T08:00:15.736819Z"},"trusted":true},"outputs":[],"source":["ds=Dataset(\"train\")\n","x, indiv_mels, mel, y=ds[0]\n","print(x.shape)\n","print(indiv_mels.shape)\n","print(mel.shape)\n","print(y.shape)"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["#### 3. 训练"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-25T08:00:17.791736Z","iopub.status.busy":"2021-07-25T08:00:17.791347Z","iopub.status.idle":"2021-07-25T08:00:17.997962Z","shell.execute_reply":"2021-07-25T08:00:17.997094Z","shell.execute_reply.started":"2021-07-25T08:00:17.791703Z"},"trusted":true},"outputs":[],"source":["#bce 交叉墒loss\n","logloss = nn.BCELoss()\n","def cosine_loss(a, v, y):\n","    d = nn.functional.cosine_similarity(a, v)\n","    loss = logloss(d.unsqueeze(1), y)\n","\n","    return loss\n","\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","syncnet = SyncNet().to(device) # 定义syncnet 模型\n","for p in syncnet.parameters():\n","    p.requires_grad = False\n","\n","    \n","#####L1 loss    \n","recon_loss = nn.L1Loss()\n","def get_sync_loss(mel, g):\n","    g = g[:, :, :, g.size(3)//2:]\n","    g = torch.cat([g[:, :, i] for i in range(syncnet_T)], dim=1)\n","    # B, 3 * T, H//2, W\n","    a, v = syncnet(mel, g)\n","    y = torch.ones(g.size(0), 1).float().to(device)\n","    return cosine_loss(a, v, y)\n","\n","def train(device, model, disc, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n","          checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n","    global global_step, global_epoch\n","    resumed_step = global_step\n","\n","    while global_epoch < nepochs:\n","        print('Starting Epoch: {}'.format(global_epoch))\n","        running_sync_loss, running_l1_loss, disc_loss, running_perceptual_loss = 0., 0., 0., 0.\n","        running_disc_real_loss, running_disc_fake_loss = 0., 0.\n","        prog_bar = tqdm(enumerate(train_data_loader))\n","        for step, (x, indiv_mels, mel, gt) in prog_bar:\n","            disc.train()\n","            model.train()\n","\n","            x = x.to(device)\n","            mel = mel.to(device)\n","            indiv_mels = indiv_mels.to(device)\n","            gt = gt.to(device)\n","\n","            ### Train generator now. Remove ALL grads. \n","            #训练生成器\n","            optimizer.zero_grad()\n","            disc_optimizer.zero_grad()\n","\n","            g = model(indiv_mels, x)#得到生成的结果\n","\n","            if hparams.syncnet_wt > 0.:\n","                sync_loss = get_sync_loss(mel, g)# 从预训练的expert 模型中获得唇音同步的损失\n","            else:\n","                sync_loss = 0.\n","\n","            if hparams.disc_wt > 0.:\n","                perceptual_loss = disc.perceptual_forward(g)#判别器的感知损失\n","            else:\n","                perceptual_loss = 0.\n","\n","            l1loss = recon_loss(g, gt)#l1 loss，重建损失\n","            \n","            #最终的损失函数\n","            loss = hparams.syncnet_wt * sync_loss + hparams.disc_wt * perceptual_loss + \\\n","                                    (1. - hparams.syncnet_wt - hparams.disc_wt) * l1loss\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            ### Remove all gradients before Training disc\n","            # 训练判别器\n","            disc_optimizer.zero_grad()\n","\n","            pred = disc(gt)\n","            disc_real_loss = F.binary_cross_entropy(pred, torch.ones((len(pred), 1)).to(device))\n","            disc_real_loss.backward()\n","\n","            pred = disc(g.detach())\n","            disc_fake_loss = F.binary_cross_entropy(pred, torch.zeros((len(pred), 1)).to(device))\n","            disc_fake_loss.backward()\n","\n","            disc_optimizer.step()\n","\n","            running_disc_real_loss += disc_real_loss.item()\n","            running_disc_fake_loss += disc_fake_loss.item()\n","\n","            # Logs\n","            global_step += 1\n","            cur_session_steps = global_step - resumed_step\n","\n","            running_l1_loss += l1loss.item()\n","            if hparams.syncnet_wt > 0.:\n","                running_sync_loss += sync_loss.item()\n","            else:\n","                running_sync_loss += 0.\n","\n","            if hparams.disc_wt > 0.:\n","                running_perceptual_loss += perceptual_loss.item()\n","            else:\n","                running_perceptual_loss += 0.\n","\n","            if global_step == 1 or global_step % checkpoint_interval == 0:\n","                save_checkpoint(\n","                    model, optimizer, global_step, checkpoint_dir, global_epoch)\n","                save_checkpoint(disc, disc_optimizer, global_step, checkpoint_dir, global_epoch, prefix='disc_')\n","\n","\n","            if global_step % hparams.eval_interval == 0:\n","                with torch.no_grad():\n","                    average_sync_loss = eval_model(test_data_loader, global_step, device, model, disc)\n","\n","                    if average_sync_loss < .75:\n","                        hparams.set_hparam('syncnet_wt', 0.03)\n","\n","            prog_bar.set_description('L1: {}, Sync: {}, Percep: {} | Fake: {}, Real: {}'.format(running_l1_loss / (step + 1),\n","                                                                                        running_sync_loss / (step + 1),\n","                                                                                        running_perceptual_loss / (step + 1),\n","                                                                                        running_disc_fake_loss / (step + 1),\n","                                                                                        running_disc_real_loss / (step + 1)))\n","\n","        global_epoch += 1\n","\n","def eval_model(test_data_loader, global_step, device, model, disc):\n","    eval_steps = 300\n","    print('Evaluating for {} steps'.format(eval_steps))\n","    running_sync_loss, running_l1_loss, running_disc_real_loss, running_disc_fake_loss, running_perceptual_loss = [], [], [], [], []\n","    while 1:\n","        for step, (x, indiv_mels, mel, gt) in enumerate((test_data_loader)):\n","            model.eval()\n","            disc.eval()\n","\n","            x = x.to(device)\n","            mel = mel.to(device)\n","            indiv_mels = indiv_mels.to(device)\n","            gt = gt.to(device)\n","\n","            pred = disc(gt)\n","            disc_real_loss = F.binary_cross_entropy(pred, torch.ones((len(pred), 1)).to(device))\n","\n","            g = model(indiv_mels, x)\n","            pred = disc(g)\n","            disc_fake_loss = F.binary_cross_entropy(pred, torch.zeros((len(pred), 1)).to(device))\n","\n","            running_disc_real_loss.append(disc_real_loss.item())\n","            running_disc_fake_loss.append(disc_fake_loss.item())\n","\n","            sync_loss = get_sync_loss(mel, g)\n","            \n","            if hparams.disc_wt > 0.:\n","                perceptual_loss = disc.perceptual_forward(g)\n","            else:\n","                perceptual_loss = 0.\n","\n","            l1loss = recon_loss(g, gt)\n","\n","            loss = hparams.syncnet_wt * sync_loss + hparams.disc_wt * perceptual_loss + \\\n","                                    (1. - hparams.syncnet_wt - hparams.disc_wt) * l1loss\n","\n","            running_l1_loss.append(l1loss.item())\n","            running_sync_loss.append(sync_loss.item())\n","            \n","            if hparams.disc_wt > 0.:\n","                running_perceptual_loss.append(perceptual_loss.item())\n","            else:\n","                running_perceptual_loss.append(0.)\n","\n","            if step > eval_steps: break\n","\n","        print('L1: {}, Sync: {}, Percep: {} | Fake: {}, Real: {}'.format(sum(running_l1_loss) / len(running_l1_loss),\n","                                                            sum(running_sync_loss) / len(running_sync_loss),\n","                                                            sum(running_perceptual_loss) / len(running_perceptual_loss),\n","                                                            sum(running_disc_fake_loss) / len(running_disc_fake_loss),\n","                                                             sum(running_disc_real_loss) / len(running_disc_real_loss)))\n","        return sum(running_sync_loss) / len(running_sync_loss)\n","\n","latest_wav2lip_checkpoint = ''\n","def save_checkpoint(model, optimizer, step, checkpoint_dir, epoch, prefix=''):\n","    global latest_wav2lip_checkpoint\n","    checkpoint_path = join(\n","        checkpoint_dir, \"{}checkpoint_step{:09d}.pth\".format(prefix, global_step))\n","    if 'disc' not in checkpoint_path:\n","        latest_wav2lip_checkpoint = checkpoint_path\n","    optimizer_state = optimizer.state_dict() if hparams.save_optimizer_state else None\n","    torch.save({\n","        \"state_dict\": model.state_dict(),\n","        \"optimizer\": optimizer_state,\n","        \"global_step\": step,\n","        \"global_epoch\": epoch,\n","    }, checkpoint_path)\n","    print(\"Saved checkpoint:\", checkpoint_path)\n","\n","def _load(checkpoint_path):\n","    if use_cuda:\n","        checkpoint = torch.load(checkpoint_path)\n","    else:\n","        checkpoint = torch.load(checkpoint_path,\n","                                map_location=lambda storage, loc: storage)\n","    return checkpoint\n","\n","\n","def load_checkpoint(path, model, optimizer, reset_optimizer=False, overwrite_global_states=True):\n","    global global_step\n","    global global_epoch\n","\n","    print(\"Load checkpoint from: {}\".format(path))\n","    checkpoint = _load(path)\n","    s = checkpoint[\"state_dict\"]\n","    new_s = {}\n","    for k, v in s.items():\n","        new_s[k.replace('module.', '')] = v\n","    model.load_state_dict(new_s)\n","    if not reset_optimizer:\n","        optimizer_state = checkpoint[\"optimizer\"]\n","        if optimizer_state is not None:\n","            print(\"Load optimizer state from {}\".format(path))\n","            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","    if overwrite_global_states:\n","        global_step = checkpoint[\"global_step\"]\n","        global_epoch = checkpoint[\"global_epoch\"]\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-25T08:01:08.453095Z","iopub.status.busy":"2021-07-25T08:01:08.452726Z","iopub.status.idle":"2021-07-25T08:01:17.172696Z","shell.execute_reply":"2021-07-25T08:01:17.169735Z","shell.execute_reply.started":"2021-07-25T08:01:08.453052Z"},"trusted":true},"outputs":[],"source":["checkpoint_dir = \"/kaggle/working/wav2lip_checkpoints\"  #checkpoint 存储的位置\n","\n","# Dataset and Dataloader setup\n","train_dataset = Dataset('train')\n","test_dataset = Dataset('val')\n","\n","train_data_loader = data_utils.DataLoader(\n","    train_dataset, batch_size=hparams.batch_size, shuffle=True,\n","    num_workers=hparams.num_workers)\n","\n","test_data_loader = data_utils.DataLoader(\n","    test_dataset, batch_size=hparams.batch_size,\n","    num_workers=4)\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n"," # Model\n","model = Wav2Lip().to(device)####### 生成器模型\n","disc = Wav2Lip_disc_qual().to(device)####### 判别器模型\n","\n","print('total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n","print('total DISC trainable params {}'.format(sum(p.numel() for p in disc.parameters() if p.requires_grad)))\n","\n","optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n","                       lr=hparams.initial_learning_rate,\n","                       betas=(0.5, 0.999))#####adam优化器，betas=[0.5,0.999]\n","disc_optimizer = optim.Adam([p for p in disc.parameters() if p.requires_grad],\n","                            lr=hparams.disc_initial_learning_rate,\n","                            betas=(0.5, 0.999))#####adam优化器，betas=[0.5,0.999]\n","\n","#继续训练的生成器的checkpoint位置\n","# checkpoint_path=\"\"\n","# load_checkpoint(checkpoint_path, model, optimizer, reset_optimizer=False)\n","#继续训练的判别器的checkpoint位置\n","# disc_checkpoint_path=\"\"\n","# load_checkpoint(disc_checkpoint_path, disc, disc_optimizer, \n","#                             reset_optimizer=False, overwrite_global_states=False)\n","\n","# syncnet的checkpoint位置，我们将使用此模型计算生成的帧和语音的唇音同步损失\n","syncnet_checkpoint_path = latest_checkpoint_path\n","# syncnet_checkpoint_path=\"/kaggle/working/expert_checkpoints/checkpoint_step000000001.pth\"\n","load_checkpoint(syncnet_checkpoint_path, syncnet, None, reset_optimizer=True,\n","                            overwrite_global_states=False)\n","\n","if not os.path.exists(checkpoint_dir):\n","    os.mkdir(checkpoint_dir)\n","\n","# Train!\n","train(device, model, disc, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n","          checkpoint_dir=checkpoint_dir,\n","          checkpoint_interval=hparams.checkpoint_interval,\n","          nepochs=5)"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["#### 4. 命令行训练\n","上面是按步骤训练的过程，在`hq_wav2lip_train.py`文件中已经把上述的过程进行了封装，你可以通过以下的命令直接进行训练"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false},"outputs":[],"source":["# !python wav2lip_train.py --data_root lrs2_preprocessed/ --checkpoint_dir <folder_to_save_checkpoints> --syncnet_checkpoint_path <path_to_expert_disc_checkpoint>"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["### 4. 模型的推理\n","当模型训练完毕后，我们只使用生成器的网络模型部分作为我们的推理模型。模型的输入由一段包含人脸的参照视频和一段语音组成。  \n","在这里我们可以直接使用官方提供给我们的预训练模型[weight](https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA?e=n9ljGW)下载该模型并放入到指定文件夹下，供之后的推理使用。  \n","模型的推理过程主要分为以下几个步骤：\n","1. 输入数据的预处理，包含人脸抠图，视频分帧，提取mel-spectrogram特征等操作。\n","2. 利用网络模型生成唇音同步的视频帧。\n","3. 将生成的视频帧准换成视频，并和输入的语音结合，形成最终的输出视频。  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-24T13:57:11.710284Z","iopub.status.busy":"2021-07-24T13:57:11.709203Z","iopub.status.idle":"2021-07-24T13:57:11.766524Z","shell.execute_reply":"2021-07-24T13:57:11.765766Z","shell.execute_reply.started":"2021-07-24T13:57:11.710238Z"},"trusted":true},"outputs":[],"source":["from os import listdir, path\n","import numpy as np\n","import scipy, cv2, os, sys, argparse, audio\n","import json, subprocess, random, string\n","from tqdm import tqdm\n","from glob import glob\n","import torch, face_detection\n","from models import Wav2Lip\n","import platform\n","import audio"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-24T13:59:02.638691Z","iopub.status.busy":"2021-07-24T13:59:02.638338Z","iopub.status.idle":"2021-07-24T13:59:02.644008Z","shell.execute_reply":"2021-07-24T13:59:02.64312Z","shell.execute_reply.started":"2021-07-24T13:59:02.638662Z"},"trusted":true},"outputs":[],"source":["checkpoint_path=\"/kaggle/working/wav2lip_checkpoints/checkpoint_step000000001.pth\"#生成器的checkpoint位置\n","checkpoint_path = latest_wav2lip_checkpoint\n","face=\"input_video.mp4\" #参照视频的文件位置, *.mp4\n","speech=\"input_audio.wav\"#输入语音的位置，*.wav\n","resize_factor=1 #对输入的视频进行下采样的倍率\n","crop=[0,-1,0,-1] #是否对视频帧进行裁剪,处理视频中有多张人脸时有用\n","fps=25#视频的帧率\n","static=False #是否只使用固定的一帧作为视频的生成参照"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-24T13:59:14.496942Z","iopub.status.busy":"2021-07-24T13:59:14.496626Z","iopub.status.idle":"2021-07-24T13:59:14.93247Z","shell.execute_reply":"2021-07-24T13:59:14.931651Z","shell.execute_reply.started":"2021-07-24T13:59:14.496913Z"},"trusted":true},"outputs":[],"source":["if not os.path.isfile(face):\n","    raise ValueError('--face argument must be a valid path to video/image file')\n","\n","\n","else:# 若输入的是视频格式\n","    video_stream = cv2.VideoCapture(face)# 读取视频\n","    fps = video_stream.get(cv2.CAP_PROP_FPS)# 读取 fps\n","\n","    print('Reading video frames...')\n","\n","    full_frames = []\n","    #提取所有的帧\n","    while 1:\n","        still_reading, frame = video_stream.read()\n","        if not still_reading:\n","            video_stream.release()\n","            break\n","        if resize_factor > 1: # 进行下采样，降低分辨率\n","            frame = cv2.resize(frame, (frame.shape[1]//resize_factor, frame.shape[0]//resize_factor))\n","\n","        \n","\n","        y1, y2, x1, x2 =crop  # 裁剪\n","        if x2 == -1: x2 = frame.shape[1]\n","        if y2 == -1: y2 = frame.shape[0]\n","\n","        frame = frame[y1:y2, x1:x2]\n","\n","        full_frames.append(frame)\n","\n","print (\"Number of frames available for inference: \"+str(len(full_frames)))"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-24T13:59:29.319869Z","iopub.status.busy":"2021-07-24T13:59:29.319525Z","iopub.status.idle":"2021-07-24T13:59:30.692164Z","shell.execute_reply":"2021-07-24T13:59:30.691135Z","shell.execute_reply.started":"2021-07-24T13:59:29.319841Z"},"trusted":true},"outputs":[],"source":["#检查输入的音频是否为 .wav格式的，若不是则进行转换\n","if not speech.endswith('.wav'):\n","    print('Extracting raw audio...')\n","    command = 'ffmpeg -y -i {} -strict -2 {}'.format(speech, 'temp/temp.wav')\n","\n","    subprocess.call(command, shell=True)\n","    speech = 'temp/temp.wav'\n","\n","wav = audio.load_wav(speech, 16000)#保证采样率为16000\n","mel = audio.melspectrogram(wav)\n","print(mel.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-24T13:59:40.598395Z","iopub.status.busy":"2021-07-24T13:59:40.598054Z","iopub.status.idle":"2021-07-24T13:59:40.610574Z","shell.execute_reply":"2021-07-24T13:59:40.609759Z","shell.execute_reply.started":"2021-07-24T13:59:40.598345Z"},"trusted":true},"outputs":[],"source":["wav2lip_batch_size=128 #推理时输入到网络的batchsize\n","mel_step_size=16\n","\n","#提取语音的mel谱\n","mel_chunks = []\n","mel_idx_multiplier = 80./fps \n","i = 0\n","while 1:\n","    start_idx = int(i * mel_idx_multiplier)\n","    if start_idx + mel_step_size > len(mel[0]):\n","        mel_chunks.append(mel[:, len(mel[0]) - mel_step_size:])\n","        break\n","    mel_chunks.append(mel[:, start_idx : start_idx + mel_step_size])\n","    i += 1\n","\n","print(\"Length of mel chunks: {}\".format(len(mel_chunks)))\n","\n","full_frames = full_frames[:len(mel_chunks)]\n","\n","batch_size = wav2lip_batch_size"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-24T13:59:59.813957Z","iopub.status.busy":"2021-07-24T13:59:59.81362Z","iopub.status.idle":"2021-07-24T13:59:59.841986Z","shell.execute_reply":"2021-07-24T13:59:59.840915Z","shell.execute_reply.started":"2021-07-24T13:59:59.813928Z"},"trusted":true},"outputs":[],"source":["img_size = 96 #默认的输入图片大小\n","pads=[0,20,0,0] # 填充的长度，保证下巴也在抠图的范围之内\n","nosmooth=False\n","face_det_batch_size=16\n","\n","def get_smoothened_boxes(boxes, T):\n","    for i in range(len(boxes)):\n","        if i + T > len(boxes):\n","            window = boxes[len(boxes) - T:]\n","        else:\n","            window = boxes[i : i + T]\n","        boxes[i] = np.mean(window, axis=0)\n","    return boxes\n","\n","#人脸检测函数\n","def face_detect(images):\n","    detector = face_detection.FaceAlignment(face_detection.LandmarksType._2D, \n","                                            flip_input=False, device=device)\n","\n","    batch_size = face_det_batch_size\n","\n","    while 1:\n","        predictions = []\n","        try:\n","            for i in tqdm(range(0, len(images), batch_size)):\n","                predictions.extend(detector.get_detections_for_batch(np.array(images[i:i + batch_size])))\n","        except RuntimeError:\n","            if batch_size == 1: \n","                raise RuntimeError('Image too big to run face detection on GPU. Please use the --resize_factor argument')\n","            batch_size //= 2\n","            print('Recovering from OOM error; New batch size: {}'.format(batch_size))\n","            continue\n","        break\n","\n","    results = []\n","    pady1, pady2, padx1, padx2 = pads\n","    for rect, image in zip(predictions, images):\n","        if rect is None:\n","            cv2.imwrite('temp/faulty_frame.jpg', image) # check this frame where the face was not detected.\n","            raise ValueError('Face not detected! Ensure the video contains a face in all the frames.')\n","\n","        y1 = max(0, rect[1] - pady1)\n","        y2 = min(image.shape[0], rect[3] + pady2)\n","        x1 = max(0, rect[0] - padx1)\n","        x2 = min(image.shape[1], rect[2] + padx2)\n","\n","        results.append([x1, y1, x2, y2])\n","\n","    boxes = np.array(results)\n","    if not nosmooth: boxes = get_smoothened_boxes(boxes, T=5)\n","    results = [[image[y1: y2, x1:x2], (y1, y2, x1, x2)] for image, (x1, y1, x2, y2) in zip(images, boxes)]\n","\n","    del detector\n","    return results \n","\n","box=[-1,-1,-1,-1]\n","\n","def datagen(frames, mels):\n","    img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n","\n","    if box[0] == -1:# 如果未指定 特定的人脸边界的话\n","        if not static:# 是否使用视频的第一帧作为参考\n","            face_det_results = face_detect(frames) # BGR2RGB for CNN face detection\n","        else:\n","            face_det_results = face_detect([frames[0]])\n","    else:\n","        print('Using the specified bounding box instead of face detection...')\n","        y1, y2, x1, x2 = box\n","        face_det_results = [[f[y1: y2, x1:x2], (y1, y2, x1, x2)] for f in frames] # 裁剪出人脸结果\n","\n","    for i, m in enumerate(mels):\n","        idx = 0 if static else i%len(frames)\n","        frame_to_save = frames[idx].copy()\n","        face, coords = face_det_results[idx].copy()\n","\n","        face = cv2.resize(face, (img_size, img_size)) # 重采样到指定大小\n","\n","        img_batch.append(face)\n","        mel_batch.append(m)\n","        frame_batch.append(frame_to_save)\n","        coords_batch.append(coords)\n","\n","        if len(img_batch) >= wav2lip_batch_size:\n","            img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n","\n","            img_masked = img_batch.copy()\n","            img_masked[:, img_size//2:] = 0\n","\n","            img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n","            mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n","\n","            yield img_batch, mel_batch, frame_batch, coords_batch\n","            img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n","\n","    if len(img_batch) > 0:\n","        img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n","\n","        img_masked = img_batch.copy()\n","        img_masked[:, img_size//2:] = 0\n","\n","        img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n","        mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n","\n","        yield img_batch, mel_batch, frame_batch, coords_batch\n","\n","mel_step_size = 16 \n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('Using {} for inference.'.format(device))\n","\n","\n","#加载模型\n","def _load(checkpoint_path):\n","    if device == 'cuda':\n","        checkpoint = torch.load(checkpoint_path)\n","    else:\n","        checkpoint = torch.load(checkpoint_path,\n","                                map_location=lambda storage, loc: storage)\n","    return checkpoint\n","\n","def load_model(path):\n","    model = Wav2Lip()\n","    print(\"Load checkpoint from: {}\".format(path))\n","    checkpoint = _load(path)\n","    s = checkpoint[\"state_dict\"]\n","    new_s = {}\n","    for k, v in s.items():\n","        new_s[k.replace('module.', '')] = v\n","    model.load_state_dict(new_s)\n","\n","    model = model.to(device)\n","    return model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false},"outputs":[],"source":["os.mkdir('/kaggle/working/temp/')"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-24T14:00:13.510396Z","iopub.status.busy":"2021-07-24T14:00:13.50993Z","iopub.status.idle":"2021-07-24T14:00:31.025133Z","shell.execute_reply":"2021-07-24T14:00:31.024237Z","shell.execute_reply.started":"2021-07-24T14:00:13.510341Z"},"trusted":true},"outputs":[],"source":["full_frames = full_frames[:len(mel_chunks)]\n","\n","batch_size = wav2lip_batch_size\n","gen = datagen(full_frames.copy(), mel_chunks)  # 进行人脸的裁剪与拼接，6通道\n","\n","for i, (img_batch, mel_batch, frames, coords) in enumerate(tqdm(gen, \n","                                        total=int(np.ceil(float(len(mel_chunks))/batch_size)))):\n","    #加载模型\n","    if i == 0:\n","        model = load_model(checkpoint_path)\n","        print (\"Model loaded\")\n","\n","        frame_h, frame_w = full_frames[0].shape[:-1]\n","        #暂存临时视频\n","        out = cv2.VideoWriter('/kaggle/working/temp/result_without_audio.mp4',\n","                                cv2.VideoWriter_fourcc(*'DIVX'), fps, (frame_w, frame_h))\n","\n","    img_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(device)\n","    mel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(device)\n","    \n","    \n","    ##### 将 img_batch, mel_batch送入模型得到pred\n","    ##############TODO##############\n","    with torch.no_grad():\n","        pred = model(mel_batch, img_batch)\n","    \n","    pred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.\n","\n","    for p, f, c in zip(pred, frames, coords):\n","        y1, y2, x1, x2 = c\n","        p = cv2.resize(p.astype(np.uint8), (x2 - x1, y2 - y1))\n","\n","        f[y1:y2, x1:x2] = p\n","        out.write(f)\n","\n","out.release()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false},"outputs":[],"source":["os.mkdir('/kaggle/working/result/')"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2021-07-24T14:01:22.990598Z","iopub.status.busy":"2021-07-24T14:01:22.990221Z","iopub.status.idle":"2021-07-24T14:01:25.928936Z","shell.execute_reply":"2021-07-24T14:01:25.927872Z","shell.execute_reply.started":"2021-07-24T14:01:22.990563Z"},"trusted":true},"outputs":[],"source":["#将生成的视频与语音合并\n","outfile=\"/kaggle/working/result/result.mp4\"# 最终输出结果到该文件夹下\n","command = 'ffmpeg -y -i {} -i {} -strict -2 -q:v 1 {}'.format(speech, '/kaggle/working/temp/result_without_audio.mp4',outfile)\n","subprocess.call(command, shell=platform.system() != 'Windows')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1480682,"sourceId":2446822,"sourceType":"datasetVersion"},{"datasetId":1486947,"sourceId":2456654,"sourceType":"datasetVersion"},{"datasetId":1506511,"sourceId":2488763,"sourceType":"datasetVersion"}],"dockerImageVersionId":30120,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
